% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model-20-performance-all.R
\name{ModelPerformance}
\alias{ModelPerformance}
\title{Evaluate and Save Model Performance Metrics}
\usage{
ModelPerformance(
  object,
  group_col = "group",
  best_threshold = NULL,
  save_dir = here::here("ModelData", "best_model_result"),
  save_data = TRUE,
  csv_filename = "performance_summary.csv"
)
}
\arguments{
\item{object}{A Best_Model object containing the model and datasets to evaluate}

\item{group_col}{Name of the response variable column (default: "group")}

\item{best_threshold}{Custom classification threshold (default: NULL uses model's optimal threshold)}

\item{save_dir}{Directory path to save CSV files (default: here::here("ModelData", "performance_results"))}

\item{save_data}{Logical indicating whether to save results to CSV (default: TRUE)}
}
\value{
The updated Best_Model object with performance results stored in @performance.result slot.
When save_data=TRUE, also saves:
- performance_summary.csv: Combined metrics across all datasets
- individual CSV files for each dataset (training_metrics.csv, etc.)
}
\description{
This function evaluates model performance across training, testing, validation,
and external validation datasets, and saves the results to CSV files.
}
\examples{
\dontrun{
# With custom parameters
object_best <- ModelPerformance(
  object = object_best,
  best_threshold = 0.4,
  save_dir = "model_performance"
)
}

}
